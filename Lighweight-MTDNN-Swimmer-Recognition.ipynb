{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.models import Sequential, Model ,load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3\n",
    "import efficientnet.tfkeras as efn \n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, StratifiedKFold, cross_validate\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_video.utils\n",
    "from keras_video import VideoFrameGenerator,SlidingFrameGenerator\n",
    "import glob\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Swimming Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'Swimming Dataset/30hz/ALL'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    'swimmer_id': {\n",
    "        0: 'Farhan', \n",
    "        1: 'Ivan', \n",
    "        2: 'Steven', \n",
    "        3: 'Dust',\n",
    "    },\n",
    "    'action_id': {\n",
    "#         0: 'Backstroke',\n",
    "        0: 'Breaststroke',\n",
    "        1: 'Drown',\n",
    "        2: 'Freestyle',\n",
    "        3: 'Safe'\n",
    "    \n",
    "    }\n",
    "}\n",
    "dataset_dict['action_alias'] = dict((g, i) for i, g in dataset_dict['action_id'].items())\n",
    "dataset_dict['swimmer_alias'] = dict((r, i) for i, r in dataset_dict['swimmer_id'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_path, ext='avi'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
    "    the data (action,identity) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            filename = filename.replace(\" \", \"_\")\n",
    "            \n",
    "#             print(filename)\n",
    "            action, identity, _= filename.split('_')\n",
    "#             print(action)\n",
    "            return dataset_dict['action_id'][int(action)], dataset_dict['swimmer_id'][int(identity)]\n",
    "        except Exception as ex:\n",
    "            return None, None\n",
    "        \n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "#     print(files)\n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "#     print(records)\n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['action', 'id',\"file\"]\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = parse_dataset(mypath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "def plot_distribution(pd_series):\n",
    "    labels = pd_series.value_counts().index.tolist()\n",
    "    counts = pd_series.value_counts().values.tolist()\n",
    "    \n",
    "    pie_plot = go.Pie(labels=labels, values=counts, hole=.3)\n",
    "    fig = go.Figure(data=[pie_plot])\n",
    "    fig.update_layout(font=dict(family=\"'Times New Roman'\",size=25),title_text='Distribution for %s' % pd_series.name)\n",
    "    \n",
    "    fig.show()\n",
    "plot_distribution(df['action'])\n",
    "plot_distribution(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "from math import floor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data generator for the multi-labeled dataset. This class should be used when training our Keras multi-output model.\n",
    "\"\"\"\n",
    "\n",
    "'''Collect images from the first part of the video clip'''\n",
    "#this function will ignore the clip that is not satisfies the sequence length value\n",
    "class SwimmerDataGenerator():\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.df = df\n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.df))\n",
    "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n",
    "        train_idx = p[:train_up_to]\n",
    "        test_idx = p[train_up_to:]\n",
    "        \n",
    "        valid_idx = p[train_up_to:]\n",
    "#         train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
    "#         train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
    "        \n",
    "        # converts alias to id\n",
    "        self.df['action_id'] = self.df['action'].map(lambda gender: dataset_dict['action_alias'][gender])\n",
    "        self.df['swimmer_id'] = self.df['id'].map(lambda race: dataset_dict['swimmer_alias'][race])\n",
    "        \n",
    "        return train_idx, valid_idx, test_idx\n",
    "    \n",
    "    def preprocess_image(self, video_path):\n",
    "        \"\"\"\n",
    "        Used to perform some minor preprocessing on the image before inputting into the network.\n",
    "        \"\"\"\n",
    "#         im = Image.open(img_path)\n",
    "#         im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "#         im = np.array(im) / 255.0\n",
    "        count = 1\n",
    "        vidObj = cv2.VideoCapture(video_path)\n",
    "        stride = 1\n",
    "        X = []\n",
    "        tmp_frames = []\n",
    "        while 1:\n",
    "            success, image = vidObj.read()\n",
    "            if success:\n",
    "                count += 1\n",
    "                if count % stride == 0:\n",
    "                    image = image.astype(np.float32)\n",
    "#                     image /=255.0\n",
    "                    image = cv2.resize(image, (img_width, img_height))\n",
    "    #                 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    tmp_frames.append(image)\n",
    "                count += 1\n",
    "                if len(tmp_frames) == seq_len:\n",
    "                    X.append(tmp_frames)\n",
    "\n",
    "                    tmp_frames = []\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                print(\"Video has defected frame\")\n",
    "                break\n",
    "            \n",
    "        X = np.squeeze(np.array(X))\n",
    "        return X\n",
    "    \n",
    "    def generate_images(self, image_idx, is_training, batch_size=16):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        images, actions, identities = [], [],[]\n",
    "        while True:\n",
    "            for idx in image_idx:\n",
    "                person = self.df.iloc[idx]\n",
    "                \n",
    "                action = person['action_id']\n",
    "                swimmer = person['swimmer_id']\n",
    "                file = person['file']\n",
    "                \n",
    "                X = self.preprocess_image(file)\n",
    "                if X is not None:\n",
    "                    actions.append(to_categorical(action, len(dataset_dict['action_id'])))\n",
    "                    identities.append(to_categorical(swimmer, len(dataset_dict['swimmer_id'])))\n",
    "                    images.append(X)\n",
    "\n",
    "                    # yielding condition\n",
    "                    if len(images) == batch_size:\n",
    "    #                     print(len(images)) \n",
    "                        yield np.array(images).astype('float32'), [np.array(actions), np.array(identities)]\n",
    "                        images, actions, identities = [], [], []\n",
    "                    \n",
    "                else : pass\n",
    "                \n",
    "            if not is_training:\n",
    "                break\n",
    "\n",
    "'''Collect images from entire video clip'''\n",
    "class SwimmerDataGeneratorAll():\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.df = df\n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.df))\n",
    "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n",
    "        train_idx = p[:train_up_to]\n",
    "        test_idx = p[train_up_to:]\n",
    "        \n",
    "        valid_idx = p[train_up_to:]\n",
    "#         train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
    "#         train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
    "        \n",
    "        # converts alias to id\n",
    "        self.df['action_id'] = self.df['action'].map(lambda gender: dataset_dict['action_alias'][gender])\n",
    "        self.df['swimmer_id'] = self.df['id'].map(lambda race: dataset_dict['swimmer_alias'][race])\n",
    "        \n",
    "        return train_idx, valid_idx, test_idx\n",
    "    \n",
    "    def count_frames(self,cap, name, force_no_headers=False):\n",
    "        framecounters = {}\n",
    "        \"\"\" Count number of frame for video\n",
    "        if it's not possible with headers \"\"\"\n",
    "        if not force_no_headers and name in framecounters:\n",
    "            return framecounters[name]\n",
    "\n",
    "        total = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "        if force_no_headers or total < 0:\n",
    "            # headers not ok\n",
    "            total = 0\n",
    "            # TODO: we're unable to use CAP_PROP_POS_FRAME here\n",
    "            # so we open a new capture to not change the\n",
    "            # pointer position of \"cap\"\n",
    "            c = cv2.VideoCapture(name)\n",
    "            while True:\n",
    "                grabbed, frame = c.read()\n",
    "                if not grabbed:\n",
    "                    # rewind and stop\n",
    "                    break\n",
    "                total += 1\n",
    "\n",
    "        # keep the result\n",
    "        framecounters[name] = total\n",
    "\n",
    "        return total\n",
    "    \n",
    "    def get_frames(self,video, nbframe,nb_channel,force_no_headers=False):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        total_frames =self.count_frames(cap, video, force_no_headers)\n",
    "#         print(total_frames)\n",
    "        orig_total = total_frames\n",
    "        if total_frames % 2 != 0:\n",
    "            total_frames += 1\n",
    "        frame_step = floor(total_frames/(nbframe-1))\n",
    "        # TODO: fix that, a tiny video can have a frame_step that is\n",
    "        # under 1\n",
    "        frame_step = max(1, frame_step)\n",
    "        frames = []\n",
    "        frame_i = 0\n",
    "\n",
    "        while True:\n",
    "            grabbed, frame = cap.read()\n",
    "            if not grabbed:\n",
    "                break\n",
    "\n",
    "            frame_i += 1\n",
    "            if frame_i == 1 or frame_i % frame_step == 0 or frame_i == orig_total:\n",
    "                # resize\n",
    "                frame = cv2.resize(frame, (img_width, img_height))\n",
    "\n",
    "                # use RGB or Grayscale ?\n",
    "                if nb_channel == 3:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                else:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "                # to np\n",
    "                frame = img_to_array(frame) \n",
    "    #             frame /=255.0\n",
    "\n",
    "                # keep frame\n",
    "                frames.append(frame)\n",
    "\n",
    "            if len(frames) == nbframe:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if not force_no_headers and len(frames) != nbframe:\n",
    "            # There is a problem here\n",
    "            # That means that frame count in header is wrong or broken,\n",
    "            # so we need to force the full read of video to get the right\n",
    "            # frame counter\n",
    "            return self.get_frames(\n",
    "                    video,\n",
    "                    nbframe,\n",
    "                    nb_channel,\n",
    "                    force_no_headers=True)\n",
    "\n",
    "        if force_no_headers and len(frames) != nbframe:\n",
    "            # and if we really couldn't find the real frame counter\n",
    "            # so we return None. Sorry, nothing can be done...\n",
    "            log.error(\"Frame count is not OK for video %s, \"\n",
    "                      \"%d total, %d extracted\" % (\n",
    "                        video, total_frames, len(frames)))\n",
    "            return None\n",
    "\n",
    "        return np.array(frames)\n",
    "\n",
    "    def generate_images(self, image_idx, is_training, batch_size=16):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        images, actions, identities = [], [],[]\n",
    "        while True:\n",
    "            for idx in image_idx:\n",
    "                person = self.df.iloc[idx]\n",
    "                \n",
    "                action = person['action_id']\n",
    "                swimmer = person['swimmer_id']\n",
    "                file = person['file']\n",
    "                X = self.get_frames(file, seq_len,CHANNELS,force_no_headers=False)\n",
    "                if X is not None:\n",
    "                    actions.append(to_categorical(action, len(dataset_dict['action_id'])))\n",
    "                    identities.append(to_categorical(swimmer, len(dataset_dict['swimmer_id'])))\n",
    "                    images.append(X)\n",
    "\n",
    "                    # yielding condition\n",
    "                    if len(images) == batch_size:\n",
    "    #                     print(len(images)) \n",
    "                        yield np.array(images).astype('float32'), [np.array(actions), np.array(identities)]\n",
    "                        images, actions, identities = [], [], []\n",
    "                    \n",
    "                else : pass\n",
    "                \n",
    "            if not is_training:\n",
    "                break\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Spatial Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(input_feature, ratio=2):\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "\n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = tf.keras.layers.Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = tf.keras.layers.Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "\n",
    "    cbam_feature = tf.keras.layers.Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    #kernel_size = 7\n",
    "    kernel_size = 3\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "\n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLearningRateScheduler(tf.keras.callbacks.History):\n",
    "    \"\"\"\n",
    "    A learning rate scheduler that relies on changes in loss function\n",
    "    value to dictate whether learning rate is decayed or not.\n",
    "    LossLearningRateScheduler has the following properties:\n",
    "    base_lr: the starting learning rate\n",
    "    lookback_epochs: the number of epochs in the past to compare with the loss function at the current epoch to determine if progress is being made.\n",
    "    decay_threshold / decay_multiple: if loss function has not improved by a factor of decay_threshold * lookback_epochs, then decay_multiple will be applied to the learning rate.\n",
    "    spike_epochs: list of the epoch numbers where you want to spike the learning rate.\n",
    "    spike_multiple: the multiple applied to the current learning rate for a spike.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr, lookback_epochs, spike_epochs = None, spike_multiple = 10, decay_threshold = 0.002, decay_multiple = 0.40, loss_type = 'val_loss'):\n",
    "\n",
    "        super(LossLearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.lookback_epochs = lookback_epochs\n",
    "        self.spike_epochs = spike_epochs\n",
    "        self.spike_multiple = spike_multiple\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay_multiple = decay_multiple\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "\n",
    "        if len(self.epoch) > self.lookback_epochs:\n",
    "\n",
    "            current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "            target_loss = self.history[self.loss_type] \n",
    "\n",
    "            loss_diff =  target_loss[-int(self.lookback_epochs)] - target_loss[-1]\n",
    "\n",
    "            if loss_diff <= np.abs(target_loss[-1]) * (self.decay_threshold * self.lookback_epochs):\n",
    "\n",
    "                print(' '.join(('Changing learning rate from', str(current_lr), 'to', str(current_lr * self.decay_multiple))))\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.decay_multiple)\n",
    "                current_lr = current_lr * self.decay_multiple\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(' '.join(('Learning rate:', str(current_lr))))\n",
    "\n",
    "            if self.spike_epochs is not None and len(self.epoch) in self.spike_epochs:\n",
    "                print(' '.join(('Spiking learning rate from', str(current_lr), 'to', str(current_lr * self.spike_multiple))))\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.spike_multiple)\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(' '.join(('Setting learning rate to', str(self.base_lr))))\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "\n",
    "\n",
    "        return tf.keras.backend.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height, img_width = 100,100\n",
    "SIZE = (img_height, img_width)\n",
    "CHANNELS = 3\n",
    "seq_len =15\n",
    "embedding_size=128\n",
    "act_class =4\n",
    "id_class =4\n",
    "Model_input_size = (seq_len, img_height, img_width, CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_TEST_SPLIT = 0.3\n",
    "data_generator = SwimmerDataGenerator(df)\n",
    "valid_idx, train_idx, test_idx = data_generator.generate_split_indexes() \n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.3\n",
    "data_generator_add = SwimmerDataGeneratorAll(df)\n",
    "valid_idx_add, train_idx_add, test_idx_add = data_generator_add.generate_split_indexes() \n",
    "\n",
    "batch_size = len(train_idx)\n",
    "valid_batch_size = len(valid_idx)\n",
    "train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)\n",
    "valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)\n",
    "X_train , y_train = next(train_gen)\n",
    "X_val , y_val = next(valid_gen)\n",
    "train_gen_all = data_generator_add.generate_images(train_idx_add, is_training=True, batch_size=batch_size)\n",
    "valid_gen_all = data_generator_add.generate_images(valid_idx_add, is_training=True, batch_size=valid_batch_size)\n",
    "X_train_all , y_train_all = next(train_gen_all)\n",
    "X_val_all , y_val_all = next(valid_gen_all)\n",
    "\n",
    "X_all = np.concatenate((X_train, X_train_all), axis=0)\n",
    "X_valid_all = np.concatenate((X_val, X_val_all), axis=0)\n",
    "\n",
    "y_train_all1 = np.concatenate((y_train[0], y_train_all[0]))\n",
    "y_train_all2 = np.concatenate((y_train[1], y_train_all[1]))\n",
    "y_train_all= np.stack((y_train_all1,y_train_all2))\n",
    "y_valid_all1 = np.concatenate((y_val[0], y_val_all[0]))\n",
    "y_valid_all2 = np.concatenate((y_val[1], y_val_all[1]))\n",
    "y_valid_all= np.stack((y_valid_all1,y_valid_all2))\n",
    "\n",
    "y_train_all = list(y_train_all)\n",
    "y_valid_all = list(y_valid_all)\n",
    "\n",
    "\n",
    "print('Number of training data: ',len(y_train_all[0]))\n",
    "print('Number of validation data: ',len(y_valid_all[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "def fusion_attention_lstm(image_input_shape,height,width):\n",
    "    y_act= Input(shape=(act_class,),name = 'input_action')\n",
    "    y_id= Input(shape=(id_class,),name = 'input_reid')\n",
    "#     input_image = Input(shape=image_input_shape)\n",
    "    input_image = Input(batch_shape=(None, seq_len,height, width, 3))\n",
    "    eff_model=efn.EfficientNetB3(input_shape=(height, width, 3),\n",
    "                                 include_top=False,\n",
    "                                 weights='noisy-student')\n",
    "    model_backbone = Model(eff_model.input,eff_model.get_layer('block7a_project_bn').output)\n",
    "    timeDistributed_layer = tf.keras.layers.TimeDistributed(model_backbone)(input_image)\n",
    "    print(\"TimeDistributed\", timeDistributed_layer.shape)\n",
    "    \n",
    "#     '''Temporal'''\n",
    "    t = tf.keras.layers.TimeDistributed(GlobalAveragePooling2D())(timeDistributed_layer)\n",
    "    t = LSTM(256, return_sequences=True, input_shape=(t.shape[1],t.shape[2]), name=\"lstm_layer_in\")(t)\n",
    "    t = SeqSelfAttention(attention_activation='sigmoid')(t)\n",
    "    avg_pool = GlobalAveragePooling1D()(t)\n",
    "    max_pool = GlobalMaxPooling1D()(t)\n",
    "    t = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    t = Dropout(0.3)(t)\n",
    "    print(\"Temporal: \", t.shape)\n",
    "    \n",
    "    '''Spatial'''\n",
    "    s = tf.math.reduce_mean(timeDistributed_layer, axis=1)  \n",
    "    s = SeparableConv2D(filters = 512, kernel_size = (3, 3), padding = 'same')(s)\n",
    "    s = spatial_attention(s)\n",
    "    s = SeparableConv2D(filters = 512, kernel_size = (3, 3), padding = 'same')(s)\n",
    "    s = spatial_attention(s)\n",
    "    s = BatchNormalization()(s)\n",
    "    a = GlobalAveragePooling2D()(s)\n",
    "    c = Dropout(0.3)(a)\n",
    "    print(\"Spatial: \", s.shape)\n",
    "        \n",
    "    '''Fusion'''\n",
    "    f = tf.keras.layers.Concatenate()([c, t])\n",
    "    f = Dropout(0.3)(f)\n",
    "    print(\"Fusion: \", f.shape)\n",
    "    \n",
    "    return f,y_act,y_id,input_image\n",
    "\n",
    "def fc_action(x,y):\n",
    "    x = Dense(1024, name=\"fusion_dense1\")(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(act_class, activation='softmax',name=\"action_output\")(x)\n",
    "    return x\n",
    "\n",
    "def fc_reid(x,y):\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)    \n",
    "    softmax = Dense(id_class, activation='softmax',name='reid_output')(x)\n",
    "    return softmax\n",
    "\n",
    "def classification_reid(x):\n",
    "    softmax = Dense(id_class, name='softmax_id')(x)\n",
    "    return softmax\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_init = 5e-4\n",
    "def create_model_fusion(image_input_shape,height,width,lr_init,compile_model):\n",
    "     \n",
    "    model,y_act,y_id,input_image = fusion_attention_lstm(image_input_shape,height,width)\n",
    "    softmax_id = fc_reid(model,y_id)\n",
    "    softmax_action = fc_action(model,y_act)\n",
    "    optimizer = tfa.optimizers.LazyAdam(lr_init)\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "    if compile_model == 1:\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=input_image, outputs=[softmax_id,softmax_action]) \n",
    "        model.compile(loss={\n",
    "                        'reid_output' :tf.keras.losses.CategoricalCrossentropy(), \n",
    "                        'action_output' :tf.keras.losses.CategoricalCrossentropy()},\n",
    "                  optimizer=optimizer,\n",
    "                    metrics={\n",
    "                        'reid_output':'accuracy',\n",
    "                        'action_output' : 'accuracy'\n",
    "                    },\n",
    "#                       loss_weights = [1, 0.8] #Weighting coefficients\n",
    "#                   loss_weights={\n",
    "#               \"\"        'reid_output':1,\n",
    "#                       'l2_loss' :0.0005,\n",
    "#                       'action_output': 1}\n",
    "                 )\n",
    "    else : \n",
    "        model = tf.keras.models.Model(inputs=input_image, outputs=[softmax_id,softmax_action]) \n",
    "    model.summary()\n",
    "  \n",
    "    return model\n",
    "model = create_model_fusion(Model_input_size,img_height, img_width,lr_init,compile_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_path = \"MODEL_MTDNN_Swim.hdf5\"\n",
    "checkpoint_path = \"multi_model\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "callback_adapt = LossLearningRateScheduler(base_lr=lr_init, lookback_epochs=6,loss_type = 'loss')\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=model_path,\n",
    "                             monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "stop = EarlyStopping(monitor='loss', patience = 10,\n",
    "                      verbose=0, mode='auto', baseline=None, \n",
    "                      restore_best_weights=False)\n",
    "callbacks = [stop, callback_adapt,checkpoint]\n",
    "\n",
    "history = model.fit(x = X_all, y = y_train_all, class_weight=None,epochs=150, batch_size = 4 , \n",
    "                    shuffle=True, validation_data=(X_valid_all,y_valid_all),verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "height_plot,width_plot=400,800\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['action_output_accuracy'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_action_output_accuracy'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=height_plot, \n",
    "                  width=width_plot,\n",
    "                  font=dict(\n",
    "                    family=\"'Times New Roman'\",size=18),\n",
    "                  title='Accuracy for Action Recognition',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['action_output_loss'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_action_output_loss'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=height_plot, \n",
    "                  width=width_plot,\n",
    "                  font=dict(\n",
    "                    family=\"'Times New Roman'\",size=18),\n",
    "                  title='Loss for Action Recognition',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()\n",
    "\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['reid_output_accuracy'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_reid_output_accuracy'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=height_plot, \n",
    "                  width=width_plot,\n",
    "                  font=dict(\n",
    "                    family=\"'Times New Roman'\",size=18),\n",
    "                  title='Accuracy for Re-identification ',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['reid_output_loss'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_reid_output_loss'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=height_plot, \n",
    "                  width=width_plot,\n",
    "                  font=dict(\n",
    "                    family=\"'Times New Roman'\",size=18),\n",
    "                  title='Loss for Re-identification',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('multi_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('multi_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = create_model_fusion(Model_input_size,img_height, img_width,lr_init,compile_model = False)\n",
    "model.load_weights(\"MODEL_MTDNN_3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[model.input],outputs=[model.layers[-2].output,model.layers[-1].output])\n",
    "# model = Model(inputs=[model.input],outputs=[model.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_TEST(video_path,seq_len):\n",
    "    \"\"\"\n",
    "    Used to perform some minor preprocessing on the image before inputting into the network.\n",
    "    \"\"\"\n",
    "#         im = Image.open(img_path)\n",
    "#         im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "#         im = np.array(im) / 255.0\n",
    "    count = 1\n",
    "    vidObj = cv2.VideoCapture(video_path)\n",
    "    stride = 1\n",
    "    X = []\n",
    "    tmp_frames = []\n",
    "    while 1:\n",
    "        success, image = vidObj.read()\n",
    "        if success:\n",
    "            count += 1\n",
    "            if count % stride == 0:\n",
    "                image = image.astype(np.float32)\n",
    "#                 image /=255.0\n",
    "                image = cv2.resize(image, (img_width, img_height))\n",
    "#                 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                tmp_frames.append(image)\n",
    "            count += 1\n",
    "            if len(tmp_frames) == seq_len:\n",
    "                X.append(tmp_frames)\n",
    "\n",
    "                tmp_frames = []\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "            print(\"Video has defected frame\")\n",
    "#                 break\n",
    "\n",
    "    X = np.squeeze(np.array(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"Multi-label-dataset/30hz/TEST\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidObj = cv2.VideoCapture(\"2_0 (42).avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, image = vidObj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "# file = \"0_1 (6).avi\"\n",
    "file=\"test-multi/1_2 (10).avi\"\n",
    "test_len = 15\n",
    "X = preprocess_TEST(file,test_len)\n",
    "# X.extend(X)\n",
    "X = np.expand_dims(X, axis=0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embedded_features = model.predict(X, verbose=1)\n",
    "# embedded_features /= np.linalg.norm(embedded_features, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(embedded_features[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=np.argmax(embedded_features,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*y_pred_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = create_model_fusion(Model_input_size,img_height, img_width,lr_init,compile_model = False)\n",
    "model.load_weights(\"MODEL_MTDNN_new.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[model.input],outputs=[model.layers[-2].output,model.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = model.predict(X_valid_all, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_id = list(dataset_dict['swimmer_alias'].keys())\n",
    "print(list_id[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict crisp classes for test set\n",
    "yhat_classes = np.argmax(yhat_probs[0],axis=1)\n",
    "yhat_probs1 = yhat_probs[0][:, 0]\n",
    "y_valid_all1=np.argmax(y_valid_all[0],axis=1)\n",
    "cr_race = classification_report(y_valid_all1, yhat_classes, target_names=['dust', 'farhan', 'ivan', 'steven'])\n",
    "print(cr_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_valid_all1, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "y_true = ['farhan', 'ivan', 'steven', 'dust']\n",
    "y_pred = ['farhan', 'ivan', 'steven', 'dust']\n",
    "df_cm = pd.DataFrame(matrix, columns=np.unique(y_true), \n",
    "                     index = np.unique(y_pred))\n",
    "# df_cm.index.name = 'Actual'\n",
    "# df_cm.columns.name = 'Predicted'\n",
    "font = {'family': 'Times New Roman', 'size': 25, }\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title('Identification confusion matrix',fontdict=font)\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## action recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict crisp classes for test set\n",
    "yhat_classes2 = np.argmax(yhat_probs[1],axis=1)\n",
    "yhat_probs = yhat_probs[1][:, 0]\n",
    "y_valid_all2=np.argmax(y_valid_all[1],axis=1)\n",
    "cr_race = classification_report(y_valid_all2, yhat_classes2, target_names=dataset_dict['action_alias'].keys())\n",
    "print(cr_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_valid_all2, yhat_classes2)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['action_alias'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion Matrixfrom sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "font = {'family': 'Times New Roman', 'size': 25, }\n",
    "y_true = ['Breaststroke', 'Drown', 'Freestyle', 'Safe']\n",
    "y_pred = ['Breaststroke', 'Drown', 'Freestyle', 'Safe']\n",
    "df_cm = pd.DataFrame(matrix, columns=np.unique(y_true), \n",
    "                     index = np.unique(y_pred))\n",
    "# df_cm.index.name = 'Actual'\n",
    "# df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.title('Action recognition confusion matrix',fontdict=font)\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradcamplusplus.gradcam import grad_cam_plus, grad_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# model = create_model_fusion(Model_input_size,img_height, img_width,lr_init,compile_model = False)\n",
    "\n",
    "model.load_weights(\"model.hdf5\")\n",
    "\n",
    "# model.load_weights('multi_model/variables/variables')\n",
    "# model = Model(inputs=model.input[0], outputs=model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras_video import VideoFrameGenerator,SlidingFrameGenerator\n",
    "\n",
    "def frame_generator(video_path,classes,NBFRAME,BS,CHANNELS,SIZE):\n",
    "    data_aug = ImageDataGenerator()\n",
    "    training_data = VideoFrameGenerator(\n",
    "        classes = classes, \n",
    "        glob_pattern = video_path,\n",
    "        nb_frames = NBFRAME,\n",
    "        shuffle = True,\n",
    "        batch_size=BS,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug,\n",
    "        use_frame_cache=False)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgwithheat(img_path, heatmap, alpha=0.4, return_array=False):\n",
    "    \"\"\"Show the image with heatmap.\n",
    "\n",
    "    Args:\n",
    "        img_path: string.\n",
    "        heatmap:  image array, get it by calling grad_cam().\n",
    "        alpha:    float, transparency of heatmap.\n",
    "        return_array: bool, return a superimposed image array or not.\n",
    "    Return:\n",
    "        None or image array.\n",
    "    \"\"\"\n",
    "    #img = cv2.imread(img_path)\n",
    "    img  = img_path\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = (heatmap*255).astype(\"uint8\")\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + img\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    imgwithheat = Image.fromarray(superimposed_img)  \n",
    "    display(imgwithheat)\n",
    "\n",
    "    if return_array:\n",
    "        return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height, img_width = 150,150\n",
    "SIZE = (img_height, img_width)\n",
    "CHANNELS = 3\n",
    "seq_len =15\n",
    "embedding_size=128\n",
    "act_class =4\n",
    "id_class =4\n",
    "Model_input_size = (seq_len, img_height, img_width, CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_files ='test3/{classname}/*.avi'\n",
    "classes = [i.split(os.path.sep)[1] for i in glob.glob('C:/Users/farha/Documents/GitHub/ConvLSTM-action-recognition/our_data/30hz/*')]\n",
    "classes.sort()\n",
    "test_files ='C:/Users/farha/Documents/GitHub/ConvLSTM-action-recognition/our_data/30hz/{classname}/*.avi'\n",
    "test_data = frame_generator(test_files,classes,seq_len,64,CHANNELS,SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][30].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_3\n",
    "batch_normalization_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "data = random.randint(0, 62)\n",
    "# data = 25\n",
    "print(data)\n",
    "x=X[0][data]*255\n",
    "heatmap_id = grad_cam_plus(model, x,branch = 'id',layer_name=\"batch_normalization\", label_name=classes,category_id=None)\n",
    "heatmap_act = grad_cam_plus(model, x,branch = 'action',layer_name=\"batch_normalization\", label_name=classes,category_id=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_frame =14\n",
    "img = X[0][data][_frame]*255\n",
    "superimposed_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "show_imgwithheat(superimposed_img, heatmap_id,alpha=0.7)\n",
    "show_imgwithheat(superimposed_img, heatmap_act,alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('drown-ivan.png',superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "class ReshapeLayer(tf.keras.layers.Layer):\n",
    "    def call(self,inputs):\n",
    "        nshape = (1) + inputs.shape[0:]\n",
    "        return tf.reshape(inputs,nshape)\n",
    "def preprocess_img(img_path, target_size=(224,224)):\n",
    "    \"\"\"Preprocess the image by reshape and normalization.\n",
    "\n",
    "    Args:\n",
    "        img_path:  A string.\n",
    "        target_size: A tuple, reshape to this size.\n",
    "    Return:\n",
    "        An image ndarray.\n",
    "    \"\"\"\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img = image.img_to_array(img)\n",
    "#     img /= 255\n",
    "#     img=np.expand_dims(img, axis=0)\n",
    "    \n",
    "    img_tensor = ReshapeLayer()(img)\n",
    "    return img_tensor\n",
    "img_path = 'test-multi/frame_14_4 (69).png'\n",
    "img = preprocess_img(img_path,target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap = grad_cam_plus(model, img, branch =\"acation\",layer_name=\"batch_normalization_3\", label_name=None,category_id=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgwithheat1(img_path, heatmap, alpha=0.4, return_array=False):\n",
    "    \"\"\"Show the image with heatmap.\n",
    "\n",
    "    Args:\n",
    "        img_path: string.\n",
    "        heatmap:  image array, get it by calling grad_cam().\n",
    "        alpha:    float, transparency of heatmap.\n",
    "        return_array: bool, return a superimposed image array or not.\n",
    "    Return:\n",
    "        None or image array.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "#     img = img_path\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = (heatmap*255).astype(\"uint8\")\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + img\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    imgwithheat = Image.fromarray(superimposed_img)  \n",
    "    display(imgwithheat)\n",
    "\n",
    "    if return_array:\n",
    "        return superimposed_img\n",
    "show_imgwithheat1(img_path, heatmap,alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
