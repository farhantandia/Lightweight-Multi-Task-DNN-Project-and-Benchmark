{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from tensorflow.keras import backend as K\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "dataset_folder_name = 'UTKFace/UTKFace'\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "IM_WIDTH = IM_HEIGHT = 198\n",
    "dataset_dict = {\n",
    "    'race_id': {\n",
    "        0: 'white', \n",
    "        1: 'black', \n",
    "        2: 'asian', \n",
    "        3: 'indian', \n",
    "        4: 'others'\n",
    "    },\n",
    "    'gender_id': {\n",
    "        0: 'male',\n",
    "        1: 'female'\n",
    "    }\n",
    "}\n",
    "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
    "dataset_dict['race_alias'] = dict((r, i) for i, r in dataset_dict['race_id'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
    "    the data (age, gender and sex) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            age, gender, race, _ = filename.split('_')\n",
    "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\n",
    "        except Exception as ex:\n",
    "            return None, None, None\n",
    "        \n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "    \n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "#     print(records)   \n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['age', 'gender', 'race', 'file']\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = parse_dataset(dataset_folder_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_distribution(pd_series):\n",
    "    labels = pd_series.value_counts().index.tolist()\n",
    "    counts = pd_series.value_counts().values.tolist()\n",
    "    \n",
    "    pie_plot = go.Pie(labels=labels, values=counts, hole=.3)\n",
    "    fig = go.Figure(data=[pie_plot])\n",
    "    fig.update_layout(title_text='Distribution for %s' % pd_series.name)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(df, x=\"age\", nbins=20)\n",
    "fig.update_layout(title_text='Age distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 20, 30, 40, 60, 80, np.inf]\n",
    "names = ['<10', '10-20', '20-30', '30-40', '40-60', '60-80', '80+']\n",
    "age_binned = pd.cut(df['age'], bins, labels=names)\n",
    "plot_distribution(age_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-labeled data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReshapeLayer(tf.keras.layers.Layer):\n",
    "    def call(self,inputs):\n",
    "        nshape = (1) + inputs.shape[0:]\n",
    "        return tf.reshape(inputs,nshape)\n",
    "class UtkFaceDataGenerator():\n",
    "    \"\"\"\n",
    "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.df))\n",
    "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n",
    "        train_idx = p[:train_up_to]\n",
    "        test_idx = p[train_up_to:]\n",
    "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
    "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
    "        \n",
    "        # converts alias to id\n",
    "        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n",
    "        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\n",
    "        self.max_age = self.df['age'].max()\n",
    "        \n",
    "        return train_idx, valid_idx, test_idx\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Used to perform some minor preprocessing on the image before inputting into the network.\n",
    "        \"\"\"\n",
    "        im = Image.open(img_path)\n",
    "        im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "        im = np.array(im)\n",
    "        im = ReshapeLayer()(im)\n",
    "        \n",
    "        return im\n",
    "        \n",
    "    def generate_images(self, image_idx, is_training, batch_size=16):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        images, ages, races, genders = [], [], [], []\n",
    "        while True:\n",
    "            for idx in image_idx:\n",
    "                person = self.df.iloc[idx]\n",
    "                \n",
    "                age = person['age']\n",
    "                race = person['race_id']\n",
    "                gender = person['gender_id']\n",
    "                file = person['file']\n",
    "                \n",
    "                im = self.preprocess_image(file)\n",
    "                \n",
    "                ages.append(age / self.max_age)\n",
    "                races.append(to_categorical(race, len(dataset_dict['race_id'])))\n",
    "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\n",
    "                images.append(im)\n",
    "                \n",
    "                # yielding condition\n",
    "                if len(images) >= batch_size:\n",
    "                    yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\n",
    "                    images, ages, races, genders = [], [], [], []\n",
    "                    \n",
    "            if not is_training:\n",
    "                break\n",
    "                \n",
    "data_generator = UtkFaceDataGenerator(df)\n",
    "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "# for x in base_model.layers[:-1]:\n",
    "#     x.trainable = True\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Spatial Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention(input_feature):\n",
    "    #kernel_size = 7\n",
    "    kernel_size = 3\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "\n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "def fusion_attention_lstm(image_input_shape,height,width):\n",
    "    seq_len =1 \n",
    "    input_image = Input(batch_shape=(None, seq_len,height, width, 3))\n",
    "    eff_model=efn.EfficientNetB3(input_shape=(height, width, 3),\n",
    "                                 include_top=False,\n",
    "                                 weights='noisy-student')\n",
    "    model_backbone = Model(eff_model.input,eff_model.get_layer('block7a_project_bn').output)\n",
    "    timeDistributed_layer = tf.keras.layers.TimeDistributed(model_backbone)(input_image)\n",
    "    print(\"TimeDistributed\", timeDistributed_layer.shape)\n",
    "    \n",
    "    '''Temporal'''\n",
    "    t = tf.keras.layers.TimeDistributed(GlobalAveragePooling2D())(timeDistributed_layer)\n",
    "    t = LSTM(256, return_sequences=True, input_shape=(t.shape[1],t.shape[2]), name=\"lstm_layer_in\")(t)\n",
    "    t = SeqSelfAttention(attention_activation='sigmoid')(t)\n",
    "    avg_pool = GlobalAveragePooling1D()(t)\n",
    "    max_pool = GlobalMaxPooling1D()(t)\n",
    "    t = concatenate([avg_pool, max_pool])\n",
    "    t = Dropout(0.3)(t)\n",
    "    print(\"Temporal: \", t.shape)\n",
    "    \n",
    "    '''Spatial'''\n",
    "    s = tf.math.reduce_mean(timeDistributed_layer, axis=1)  \n",
    "    s = SeparableConv2D(filters = 512, kernel_size = (3, 3), padding = 'same')(s)\n",
    "    s = spatial_attention(s)\n",
    "    s = SeparableConv2D(filters = 512, kernel_size = (3, 3), padding = 'same')(s)\n",
    "    s = spatial_attention(s)\n",
    "    s = BatchNormalization()(s)\n",
    "    a = GlobalAveragePooling2D()(s)\n",
    "    c = Dropout(0.3)(a)\n",
    "    print(\"Spatial: \", s.shape)\n",
    "    \n",
    "    \n",
    "    '''Fusion'''\n",
    "    f = tf.keras.layers.Concatenate()([c, t])\n",
    "    f = Dropout(0.3)(f)\n",
    "    print(\"Fusion: \", f.shape)\n",
    "    \n",
    "    return f,input_image\n",
    "\n",
    "def build_race_branch(num_races,newModel):\n",
    "\n",
    "    x = Dense(512)(newModel)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_races)(x)\n",
    "    x = Activation(\"softmax\", name=\"race_output\")(x)\n",
    "    return x\n",
    "def build_gender_branch(newModel,num_genders=2):\n",
    "\n",
    "#     x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
    "    x = Dense(512)(newModel)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_genders)(x)\n",
    "    x = Activation(\"sigmoid\", name=\"gender_output\")(x)\n",
    "    return x\n",
    "def build_age_branch(newModel):   \n",
    "\n",
    "#     x = newModel.output\n",
    "#     x = Flatten()(x)\n",
    "    x = Dense(512)(newModel)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation(\"linear\", name=\"age_output\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def assemble_full_model(width, height, num_races):\n",
    "\n",
    "    input_shape = (1,height, width, 3)\n",
    "    model,input_image = fusion_attention_lstm(input_shape,height,width)\n",
    "    \n",
    "    age_branch = build_age_branch(model)\n",
    "    race_branch = build_race_branch(num_races,model)\n",
    "    gender_branch = build_gender_branch(model)\n",
    "    model = Model(inputs=input_image,outputs = [age_branch, race_branch,gender_branch],name=\"far_net\")\n",
    "    return model\n",
    "    \n",
    "model = assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLearningRateScheduler(tf.keras.callbacks.History):\n",
    "    \"\"\"\n",
    "    A learning rate scheduler that relies on changes in loss function\n",
    "    value to dictate whether learning rate is decayed or not.\n",
    "    LossLearningRateScheduler has the following properties:\n",
    "    base_lr: the starting learning rate\n",
    "    lookback_epochs: the number of epochs in the past to compare with the loss function at the current epoch to determine if progress is being made.\n",
    "    decay_threshold / decay_multiple: if loss function has not improved by a factor of decay_threshold * lookback_epochs, then decay_multiple will be applied to the learning rate.\n",
    "    spike_epochs: list of the epoch numbers where you want to spike the learning rate.\n",
    "    spike_multiple: the multiple applied to the current learning rate for a spike.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr, lookback_epochs, spike_epochs = None, spike_multiple = 10, decay_threshold = 0.002, decay_multiple = 0.70, loss_type = 'val_loss'):\n",
    "\n",
    "        super(LossLearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.lookback_epochs = lookback_epochs\n",
    "        self.spike_epochs = spike_epochs\n",
    "        self.spike_multiple = spike_multiple\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay_multiple = decay_multiple\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "\n",
    "        if len(self.epoch) > self.lookback_epochs:\n",
    "\n",
    "            current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "            target_loss = self.history[self.loss_type] \n",
    "\n",
    "            loss_diff =  target_loss[-int(self.lookback_epochs)] - target_loss[-1]\n",
    "\n",
    "            if loss_diff <= np.abs(target_loss[-1]) * (self.decay_threshold * self.lookback_epochs):\n",
    "\n",
    "                print(' '.join(('Changing learning rate from', str(current_lr), 'to', str(current_lr * self.decay_multiple))))\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.decay_multiple)\n",
    "                current_lr = current_lr * self.decay_multiple\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(' '.join(('Learning rate:', str(current_lr))))\n",
    "\n",
    "            if self.spike_epochs is not None and len(self.epoch) in self.spike_epochs:\n",
    "                print(' '.join(('Spiking learning rate from', str(current_lr), 'to', str(current_lr * self.spike_multiple))))\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.spike_multiple)\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(' '.join(('Setting learning rate to', str(self.base_lr))))\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "\n",
    "\n",
    "        return tf.keras.backend.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "lr_init = 1e-4\n",
    "epochs = 200\n",
    "opt = tfa.optimizers.LazyAdam(lr=lr_init)\n",
    "model.compile(optimizer=opt, \n",
    "              loss={\n",
    "                  'age_output': 'mse', \n",
    "                  'race_output': tf.keras.losses.squared_hinge, \n",
    "#                   'gender_output': 'binary_crossentropy'},\n",
    "                  'gender_output': 'binary_crossentropy'},\n",
    "              loss_weights={\n",
    "#                   'age_output': 4., \n",
    "#                   'race_output': 1.5, \n",
    "#                   'gender_output': 0.1\n",
    "              },\n",
    "              metrics={\n",
    "                  'age_output': 'mae', \n",
    "                  'race_output': 'accuracy',\n",
    "                  'gender_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "batch_size = 32\n",
    "valid_batch_size = 32\n",
    "train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)\n",
    "valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = EarlyStopping(monitor='val_loss', patience = 7,\n",
    "                      verbose=0, mode='auto', baseline=None, \n",
    "                      restore_best_weights=False)\n",
    "checkpoint =  ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "callback_adapt = LossLearningRateScheduler(base_lr=lr_init, lookback_epochs=6,loss_type = 'val_loss')\n",
    "\n",
    "callbacks = [stop, callback_adapt,checkpoint]\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=len(train_idx)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['race_output_accuracy'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_race_output_accuracy'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700,\n",
    "                  title='Accuracy for race feature',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['gender_output_accuracy'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_gender_output_accuracy'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700,\n",
    "                  title='Accuracy for gender feature',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(\n",
    "                    y=history.history['age_output_loss'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scattergl(\n",
    "                    y=history.history['val_age_output_loss'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700,\n",
    "                  title='Mean Absolute Error for age feature',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Mean Absolute Error')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scattergl(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700,\n",
    "                  title='Overall loss',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 128\n",
    "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)\n",
    "age_pred, race_pred, gender_pred = model.predict_generator(test_generator, \n",
    "                                                           steps=len(test_idx)//test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)\n",
    "samples = 0\n",
    "images, age_true, race_true, gender_true = [], [], [], []\n",
    "for test_batch in test_generator:\n",
    "    image = test_batch[0]\n",
    "    labels = test_batch[1]\n",
    "    \n",
    "    images.extend(image)\n",
    "    age_true.extend(labels[0])\n",
    "    race_true.extend(labels[1])\n",
    "    gender_true.extend(labels[2])\n",
    "    \n",
    "age_true = np.array(age_true)\n",
    "race_true = np.array(race_true)\n",
    "gender_true = np.array(gender_true)\n",
    "race_true, gender_true = race_true.argmax(axis=-1), gender_true.argmax(axis=-1)\n",
    "race_pred, gender_pred = race_pred.argmax(axis=-1), gender_pred.argmax(axis=-1)\n",
    "age_true = age_true * data_generator.max_age\n",
    "age_pred = age_pred * data_generator.max_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr_race = classification_report(race_true, race_pred, target_names=dataset_dict['race_alias'].keys())\n",
    "print(cr_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_gender = classification_report(gender_true, gender_pred, target_names=dataset_dict['gender_alias'].keys())\n",
    "print(cr_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('R2 score for age: ', r2_score(age_true, age_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
